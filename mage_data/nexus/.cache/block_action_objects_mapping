{"block_file": {"charts/unique_values_for_marvelous_inventor.py:chart:python:unique values for marvelous inventor": {"content": "columns = df_1.columns\nnumber_of_unique_values = [df_1[col].nunique() for col in columns]\n", "file_path": "charts/unique_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "unique_values_for_marvelous_inventor"}, "charts/missing_values_for_marvelous_inventor.py:chart:python:missing values for marvelous inventor": {"content": "number_of_rows = len(df_1.index)\ncolumns_with_mising_values = []\npercentage_of_missing_values = []\nfor col in df_1.columns:\n    missing = df_1[col].isna().sum()\n    if missing > 0:\n        columns_with_mising_values.append(col)\n        percentage_of_missing_values.append(100 * missing / number_of_rows)\n", "file_path": "charts/missing_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "missing_values_for_marvelous_inventor"}, "data_exporters/revered_grace.py:data_exporter:python:revered grace": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-datascience.scg_stocks.EOD'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/revered_grace.py", "language": "python", "type": "data_exporter", "uuid": "revered_grace"}, "data_exporters/autumn_illusion.py:data_exporter:python:autumn illusion": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-daatascience.your_dataset.your_table_name'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    print(config_path)\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/autumn_illusion.py", "language": "python", "type": "data_exporter", "uuid": "autumn_illusion"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/victorious_inventor.yaml:data_loader:yaml:victorious inventor": {"content": "config:\n  columns: null\n  headers: null\n  method: GET\n  payload: null\n  query: null\n  response_parser: null\n  url: null\nsource: api\n", "file_path": "data_loaders/victorious_inventor.yaml", "language": "yaml", "type": "data_loader", "uuid": "victorious_inventor"}, "data_loaders/effortless_sound.py:data_loader:python:effortless sound": {"content": "import io\nfrom fredapi import Fred\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\nfrom fredapi import Fred\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\"\"\"\nFederal Reserve Bank Data Downloader\n\nA clean, pythonic module for downloading FRB rates, series, and indices data.\nDesigned to be modular and pipeline-ready.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union\nimport pandas as pd\nfrom fredapi import Fred\n\n\n@dataclass\nclass FRBConfig:\n    \"\"\"Configuration for FRB data downloads.\"\"\"\n    \n    api_key: str = get_secret_value('FRB_API_KEY')\n    data_dir: Path = Path(\"data\")\n    start_date: str = \"2010-01-01\"\n    \n    # FRB Series codes\n    series: List[str] = None\n    rates: List[str] = None\n    indices: Dict[str, str] = None\n    \n    def __post_init__(self):\n        \"\"\"Set default values after initialization.\"\"\"\n        if self.series is None:\n            self.series = [\n                'BAMLHYH0A0HYM2TRIV',  # High Yield Bond Index\n                'CPIAUCSL',            # Consumer Price Index\n                'DFF',                 # Federal Funds Rate\n                'DAAA',               # AAA Corporate Bond Yield\n                'T10Y2Y',             # 10Y-2Y Treasury Spread\n                'T10Y3M',             # 10Y-3M Treasury Spread\n                'PCUOMFGOMFG',        # Producer Price Index\n                'UNRATE',             # Unemployment Rate\n                'CIVPART',            # Labor Force Participation Rate\n                'TCU',                # Capacity Utilization\n                'INDPRO'              # Industrial Production Index\n            ]\n        \n        if self.rates is None:\n            self.rates = [\n                'DGS3MO',  # 3-Month Treasury\n                'DGS6MO',  # 6-Month Treasury\n                'DGS1',    # 1-Year Treasury\n                'DGS2',    # 2-Year Treasury\n                'DGS3',    # 3-Year Treasury\n                'DGS10'    # 10-Year Treasury\n            ]\n        \n        if self.indices is None:\n            self.indices = {\n                'SP500': 'GSPC',        # S&P 500\n                'DJIA': 'DJI',          # Dow Jones Industrial Average\n                'NASDAQCOM': 'IXIC'     # NASDAQ Composite\n            }\n        \n        # Ensure data directory exists\n        self.data_dir.mkdir(exist_ok=True)\n\n\nclass FRBDataDownloader:\n    \"\"\"Downloads and manages Federal Reserve Bank economic data.\"\"\"\n    \n    def __init__(self, config: Optional[FRBConfig] = None):\n        \"\"\"Initialize the downloader with configuration.\"\"\"\n        self.config = config or FRBConfig()\n        self.fred = Fred(api_key=self.config.api_key)\n    \n    def download_series(\n        self, \n        series_codes: List[str], \n        save_individual: bool = True,\n        save_combined: bool = False,\n        combined_filename: str = \"frb_series.csv\",\n        file_prefix: str = \"\"\n    ) -> pd.DataFrame:\n        \"\"\"\n        Download FRB series data.\n        \n        Args:\n            series_codes: List of FRED series codes to download\n            save_individual: Save each series as individual CSV file\n            save_combined: Save all series in one combined CSV file\n            combined_filename: Name for combined file\n            file_prefix: Prefix for individual files\n            \n        Returns:\n            DataFrame with all series data\n        \"\"\"\n        series_data = []\n        \n        for code in series_codes:\n            try:\n                print(f\"Downloading {code}...\")\n                series = self.fred.get_series(code)\n                \n                # Filter by start date\n                filtered_series = series[series.index >= self.config.start_date]\n                series_data.append(filtered_series)\n                \n                # Save individual file if requested\n                if save_individual:\n                    filename = f\"{file_prefix}{code}.csv\"\n                    filepath = self.config.data_dir / filename\n                    filtered_series.to_csv(filepath)\n                    \n            except Exception as e:\n                print(f\"Error downloading {code}: {e}\")\n                continue\n        \n        # Combine all series\n        if series_data:\n            combined_df = pd.concat(series_data, axis=1)\n            combined_df.columns = series_codes[:len(series_data)]\n            \n            # Save combined file if requested\n            if save_combined:\n                filepath = self.config.data_dir / combined_filename\n                combined_df.to_csv(filepath)\n            \n            return combined_df\n        \n        return pd.DataFrame()\n    \n    def download_rates(self) -> pd.DataFrame:\n        \"\"\"Download FRB treasury rates.\"\"\"\n        print(\"Downloading FRB rates...\")\n        return self.download_series(\n            series_codes=self.config.rates,\n            save_individual=True,\n            save_combined=True,\n            combined_filename=\"frb_rates.csv\",\n            file_prefix=\"rate_\"\n        )\n    \n    def download_economic_series(self) -> pd.DataFrame:\n        \"\"\"Download FRB economic indicators.\"\"\"\n        print(\"Downloading FRB economic series...\")\n        return self.download_series(\n            series_codes=self.config.series,\n            save_individual=True,\n            save_combined=True,\n            combined_filename=\"frb_economic_series.csv\",\n            file_prefix=\"series_\"\n        )\n    \n    def download_indices(self, use_ohlc_format: bool = True) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Download market indices from FRB.\n        \n        Args:\n            use_ohlc_format: Format data to match OHLC structure\n            \n        Returns:\n            Dictionary mapping index names to DataFrames\n        \"\"\"\n        print(\"Downloading market indices...\")\n        indices_data = {}\n        \n        for frb_code, yahoo_symbol in self.config.indices.items():\n            try:\n                print(f\"Downloading {frb_code} ({yahoo_symbol})...\")\n                series = self.fred.get_series(frb_code)\n                \n                # Filter by start date and create DataFrame\n                filtered_series = series[series.index >= self.config.start_date]\n                df = filtered_series.reset_index()\n                df.columns = ['Date', 'Close']\n                df.dropna(inplace=True)\n                \n                # Add OHLC columns if requested (for compatibility)\n                if use_ohlc_format:\n                    df['Open'] = pd.NA\n                    df['High'] = pd.NA\n                    df['Low'] = pd.NA\n                    df['Volume'] = pd.NA\n                    df['Dividends'] = pd.NA\n                    df['Stock Splits'] = pd.NA\n                    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n                \n                # Save to file\n                filename = f\"{yahoo_symbol}_data.csv\"\n                filepath = self.config.data_dir / filename\n                df.to_csv(filepath, index=False)\n                \n                indices_data[frb_code] = df\n                \n            except Exception as e:\n                print(f\"Error downloading {frb_code}: {e}\")\n                continue\n        \n        return indices_data\n    \n    def download_all(self) -> pd.DataFrame:\n        \"\"\"\n        Download all FRB data: rates, series, and indices.\n        \n        Returns:\n            Combined DataFrame with all data\n        \"\"\"\n        dataframes_to_combine = []\n        \n        # Download rates\n        try:\n            rates_df = self.download_rates()\n            if not rates_df.empty:\n                dataframes_to_combine.append(rates_df)\n        except Exception as e:\n            print(f\"Error downloading rates: {e}\")\n        \n        # Download economic series\n        try:\n            series_df = self.download_economic_series()\n            if not series_df.empty:\n                dataframes_to_combine.append(series_df)\n        except Exception as e:\n            print(f\"Error downloading economic series: {e}\")\n        \n        # Download indices and extract close prices\n        try:\n            indices_dict = self.download_indices()\n            if indices_dict:\n                # Extract close prices from each index and create DataFrame\n                indices_data = {}\n                for frb_code, df in indices_dict.items():\n                    if not df.empty:\n                        # Set Date as index and extract Close column\n                        df_indexed = df.set_index('Date')['Close']\n                        indices_data[frb_code] = df_indexed\n                \n                if indices_data:\n                    indices_df = pd.DataFrame(indices_data)\n                    dataframes_to_combine.append(indices_df)\n        except Exception as e:\n            print(f\"Error downloading indices: {e}\")\n        \n        # Combine all dataframes\n        if dataframes_to_combine:\n            combined_df = pd.concat(dataframes_to_combine, axis=1, sort=True)\n            return combined_df\n        else:\n            print(\"No data was successfully downloaded\")\n            return pd.DataFrame()\n\n\ndef main():\n    \"\"\"Main execution function.\"\"\"\n    # Create downloader with default configuration\n    downloader = FRBDataDownloader()\n    \n    # Download all data\n    data = downloader.download_all()\n    \n    print(\"\\nDownload Summary:\")\n    print(f\"Rates: {len(data['rates'].columns)} series downloaded\")\n    print(f\"Economic Series: {len(data['series'].columns)} series downloaded\") \n    print(f\"Indices: {len(data['indices'])} indices downloaded\")\n    print(f\"Data saved to: {downloader.config.data_dir}\")\n\n@data_loader\ndef main():\n    downloader = FRBDataDownloader()\n    data = downloader.download_all()\n    print(data.tail())", "file_path": "data_loaders/effortless_sound.py", "language": "python", "type": "data_loader", "uuid": "effortless_sound"}, "data_loaders/quixotic_core.py:data_loader:python:quixotic core": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/quixotic_core.py", "language": "python", "type": "data_loader", "uuid": "quixotic_core"}, "data_loaders/verdant_phoenix.py:data_loader:python:verdant phoenix": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    query = 'your_gbq_query'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    return BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/verdant_phoenix.py", "language": "python", "type": "data_loader", "uuid": "verdant_phoenix"}, "data_loaders/marvelous_inventor.py:data_loader:python:marvelous inventor": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nimport intrinio_sdk as intrinio\nfrom intrinio_sdk.rest import ApiException\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n#get Intrinio API Key secret\nAPI_KEY = get_secret_value('Intrinio_API')\n\ntoday = datetime.now()\nfive_years_ago = (today - relativedelta(years=3)).strftime(\"%Y-%m-%d\")\nresults_dictionary = {}\n\nintrinio.ApiClient().set_api_key(API_KEY) \nintrinio.ApiClient().allow_retries(True)\n\ndef get_universe():\n    date = '2025-05-14'  # Yesterday's date\n    page_size = 100\n    start_time = datetime.now()\n\n    # Get Marketcap data with pagination\n    marketcap_data = []\n    next_page = ''\n\n    while True:\n        try:\n            response = intrinio.CompanyApi().get_all_companies_daily_metrics(\n                on_date=date, \n                page_size=page_size,\n                next_page=next_page\n            )\n            \n            print(f'Processing {len(response.daily_metrics)} marketcap entries...')\n            \n            # Process current page of results\n            for daily_metric in response.daily_metrics:\n                ticker = daily_metric.company.ticker\n                marketcap = daily_metric.market_cap\n                \n                data = {\n                    \"ticker\" : ticker, \n                    \"name\": daily_metric.company.name,\n                    \"date\": daily_metric.date.strftime(\"%Y-%m-%d\"),\n                    \"marketcap\": marketcap\n                }\n                \n                marketcap_data.append(data)\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        except ApiException as e:\n            print(f\"Exception when calling CompanyApi->get_all_companies_daily_metrics: {e}\")\n            break\n\n    print(f'Found {len(marketcap_data)} marketcap entries for: {date}')\n    print(f'Time elapsed: {datetime.now() - start_time}')\n\n    universe = pd.DataFrame(list(marketcap_data))\n    selected_universe = universe[((universe.marketcap>2000000000) & (universe.ticker.notna()))]\n    \n    return selected_universe\n\ndef work(ticker):\n    # Get EOD Stock Prices with pagination\n    # https://docs.intrinio.com/documentation/python/get_security_stock_prices_v2\n    identifier = ticker\n    start_date = five_years_ago\n    page_size = 100  # Maximum allowed page size\n    total_prices = 0\n    \n    try:\n        # Initialize pagination\n        next_page = ''\n        \n        while True:\n            # Get the current page of results\n            response = intrinio.SecurityApi().get_security_stock_prices(\n                identifier, \n                start_date=start_date, \n                page_size=page_size, \n                next_page=next_page\n            )\n            \n            security = response.security\n            page_prices = len(response.stock_prices)\n            total_prices += page_prices\n            \n            # Process the current page of stock price data\n            for stock_price in response.stock_prices:\n                key = f'{ticker}|{stock_price.date}'\n                data = {\n                    'security_id': security.id,\n                    'company_id': security.company_id,\n                    'ticker': security.ticker,\n                    'date': stock_price.date,\n                    'open': stock_price.open,\n                    'high': stock_price.high,\n                    'low': stock_price.low,\n                    \"close\": stock_price.close,\n                    'adj_open': stock_price.adj_open,\n                    'adj_high': stock_price.adj_high,\n                    \"adj_low\": stock_price.adj_low,\n                    \"adj_close\": stock_price.adj_close,\n                    'adj_volume':stock_price.adj_volume,\n                    'fifty_two_week_high' : stock_price.fifty_two_week_high,\n                    'fifty_two_week_low':stock_price.fifty_two_week_low,\n                    'dividend': stock_price.dividend\n                }\n                results_dictionary[key] = data\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        print(f'Found {total_prices} prices for: {ticker}')\n        \n    except ApiException as e:\n        print(f\"Exception when calling SecurityApi->get_security_stock_prices for {ticker}: {e}\")\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n\n    start_time = datetime.now()\n\n    selected_universe = get_universe()\n    tickers = selected_universe.ticker.tolist()\n    \n    print(f'Loading {len(tickers)} tickers') \n    print(tickers)\n\n    for ticker in tqdm(tickers):\n        work(ticker)\n \n    df = pd.DataFrame(list(results_dictionary.values()))\n    print(len(df.ticker.unique()))\n    \n    return(df)", "file_path": "data_loaders/marvelous_inventor.py", "language": "python", "type": "data_loader", "uuid": "marvelous_inventor"}, "data_loaders/icy_echo.py:data_loader:python:icy echo": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/icy_echo.py", "language": "python", "type": "data_loader", "uuid": "icy_echo"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/purple_song/interactions.yaml:pipeline:yaml:purple song/interactions": {"content": "blocks: {}\n", "file_path": "pipelines/purple_song/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "purple_song/interactions"}, "pipelines/purple_song/__init__.py:pipeline:python:purple song/  init  ": {"content": "", "file_path": "pipelines/purple_song/__init__.py", "language": "python", "type": "pipeline", "uuid": "purple_song/__init__"}, "pipelines/purple_song/metadata.yaml:pipeline:yaml:purple song/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - revered_grace\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: effortless sound\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: effortless_sound\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/revered_grace.py\n    file_source:\n      path: data_exporters/revered_grace.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: revered_grace\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - effortless_sound\n  uuid: revered_grace\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-17 17:02:49.333987+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: purple song\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: purple_song\nvariables:\n  bigquery_table_id: scg-datascience.scg_stocks.econ\nvariables_dir: /home/src/mage_data/nexus\nwidgets: []\nvariables:\n  bigquery_table_id: \"scg-datascience.scg_stocks.econ\"\n", "file_path": "pipelines/purple_song/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "purple_song/metadata"}, "pipelines/lucid_sun/interactions.yaml:pipeline:yaml:lucid sun/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "pipelines/lucid_sun/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "lucid_sun/interactions"}, "pipelines/lucid_sun/__init__.py:pipeline:python:lucid sun/  init  ": {"content": "", "file_path": "pipelines/lucid_sun/__init__.py", "language": "python", "type": "pipeline", "uuid": "lucid_sun/__init__"}, "pipelines/lucid_sun/metadata.yaml:pipeline:yaml:lucid sun/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - revered_grace\n  - missing_values_for_marvelous_inventor\n  - unique_values_for_marvelous_inventor\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: marvelous inventor\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: marvelous_inventor\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dynamic: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: revered grace\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: revered_grace\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-14 17:20:18.207723+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: lucid sun\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: lucid_sun\nvariables_dir: /home/src/mage_data/nexus\nwidgets:\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    group_by: []\n    x: columns_with_mising_values\n    y: percentage_of_missing_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: missing values for marvelous_inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: missing_values_for_marvelous_inventor\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    x: columns\n    y: number_of_unique_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: unique values for marvelous_inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: unique_values_for_marvelous_inventor\n", "file_path": "pipelines/lucid_sun/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "lucid_sun/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "/home/src/nexus/data_loaders/effortless_sound.py:data_loader:python:home/src/nexus/data loaders/effortless sound": {"content": "import io\nfrom fredapi import Fred\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\nfrom fredapi import Fred\nimport numpy as np\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Configuration\nFRB_API_KEY = get_secret_value('Fred_API')\n\n\n# Define what we want to download and their mappings\nFRB_RATES = ['DGS10', 'DGS2', 'DGS3', 'DGS1', 'DGS3MO', 'DGS6MO']\n\nFRB_SERIES_MAPPING = {\n    'CPIAUCSL': 'CPI',\n    'PCUOMFGOMFG': 'ppi', \n    'UNRATE': 'unemprate',\n    'CIVPART': 'laborpart',\n    'INDPRO': 'indprod',\n    'DFF': 'ffr',\n    'T10Y3M': 'ycurve10y3m',\n    'T10Y2Y': 'ycurve10y2y',\n    'DAAA': 'moodycorpbondyield'\n}\n\nFRB_INDICES_MAPPING = {\n    'SP500': 'snp500'\n}\n\n\ndef download_fred_data(series_codes: list, api_key: str = FRB_API_KEY, start_date: str = \"2010-01-01\") -> pd.DataFrame:\n    \"\"\"Download multiple FRED series and return as DataFrame.\"\"\"\n    fred = Fred(api_key=api_key)\n    data_dict = {}\n    \n    for code in series_codes:\n        try:\n            print(f\"Downloading {code}\")\n            series = fred.get_series(code)\n            filtered_series = series[series.index >= start_date]\n            data_dict[code] = filtered_series\n        except Exception as e:\n            print(f\"Error downloading {code}: {e}\")\n    \n    if data_dict:\n        df = pd.concat(data_dict, axis=1)\n        df.reset_index(inplace=True)\n        df.rename(columns={'index': 'DATE'}, inplace=True)\n        return df\n    else:\n        return pd.DataFrame()\n\n\ndef rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Rename columns to match desired output format.\"\"\"\n    # Rename rate columns (convert to lowercase for some)\n    rate_renames = {\n        'DGS3MO': 'DGS3mo',\n        'DGS6MO': 'DGS6mo'\n    }\n    \n    # Combine all rename mappings\n    all_renames = {**rate_renames, **FRB_SERIES_MAPPING, **FRB_INDICES_MAPPING}\n    \n    return df.rename(columns=all_renames)\n\n\ndef calculate_derived_variables(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Calculate derived/interpolated variables.\"\"\"\n    df = df.copy()\n    \n    # Convert DGS values from percentages to decimals\n    dgs_cols = ['DGS10', 'DGS2', 'DGS3', 'DGS1', 'DGS3mo', 'DGS6mo']\n    for col in dgs_cols:\n        if col in df.columns:\n            df[col] = df[col] / 100\n    \n    # Forward fill S&P 500 data\n    df['snp500'] = df['snp500'].fillna(method='ffill')\n    \n    # Interpolated rates (now in decimal form)\n    df['dgs1p5'] = (df['DGS1'] + df['DGS2']) / 2  # 1.5-Year Rate\n    df['dgs2p5'] = (df['DGS2'] + df['DGS3']) / 2  # 2.5-Year Rate  \n    df['dgs9mo'] = (df['DGS6mo'] + df['DGS1']) / 2  # 9-Month Rate\n    df['dgs15mo'] = (df['dgs1p5'] + df['DGS1']) / 2  # 15-Month Rate\n    \n    # S&P 500 log returns (calculated after forward filling)\n    df['snp500rets'] = np.log(df['snp500']).diff()\n    \n    return df\n\n@data_loader\ndef main() -> pd.DataFrame:\n    \"\"\"Download and process all financial data.\"\"\"\n    print(\"Downloading FRB rates...\")\n    rates_df = download_fred_data(FRB_RATES)\n    \n    print(\"\\nDownloading FRB economic series...\")\n    series_df = download_fred_data(list(FRB_SERIES_MAPPING.keys()))\n    \n    print(\"\\nDownloading indices...\")\n    indices_df = download_fred_data(list(FRB_INDICES_MAPPING.keys()))\n    \n    # Merge all dataframes\n    print(\"\\nMerging data...\")\n    dataframes = [df for df in [rates_df, series_df, indices_df] if not df.empty]\n    \n    if not dataframes:\n        print(\"No data downloaded\")\n        return pd.DataFrame()\n    \n    # Start with first dataframe and merge others\n    merged = dataframes[0].copy()\n    for df in dataframes[1:]:\n        merged = pd.merge(merged, df, on='DATE', how='outer')\n    \n    # Rename columns to match desired format\n    merged = rename_columns(merged)\n    \n    # Calculate derived variables\n    print(\"Calculating derived variables...\")\n    merged = calculate_derived_variables(merged)\n    \n    # Sort by date\n    merged.sort_values('DATE', inplace=True)\n    merged.reset_index(drop=True, inplace=True)\n    \n    print(f\"\\nDownload complete! Dataset shape: {merged.shape}\")\n    print(f\"Date range: {merged['DATE'].min()} to {merged['DATE'].max()}\")\n    \n    return merged\n", "file_path": "/home/src/nexus/data_loaders/effortless_sound.py", "language": "python", "type": "data_loader", "uuid": "effortless_sound"}, "/home/src/nexus/transformers/dawn_quest.py:transformer:python:home/src/nexus/transformers/dawn quest": {"content": "from mage_ai.data_cleaner.transformer_actions.constants import ImputationStrategy\nfrom mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    print(df.head())\n    \"\"\"\n    Execute Transformer Action: ActionType.IMPUTE with SEQ for Forward Fill\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#fill-in-missing-values\n    \"\"\"\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.IMPUTE,\n        arguments=df.columns,  # Specify columns to impute\n        axis=Axis.COLUMN,\n        options={'timeseries_index': 'DATE', 'strategy': ImputationStrategy.SEQ}  # Changed to forward fill\n    )\n\n    return BaseAction(action).execute(df)\n", "file_path": "/home/src/nexus/transformers/dawn_quest.py", "language": "python", "type": "transformer", "uuid": "dawn_quest"}, "/home/src/nexus/data_exporters/revered_grace.py:data_exporter:python:home/src/nexus/data exporters/revered grace": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = kwargs['bigquery_table_id']\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "/home/src/nexus/data_exporters/revered_grace.py", "language": "python", "type": "data_exporter", "uuid": "revered_grace"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}
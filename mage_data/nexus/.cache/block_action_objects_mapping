{"block_file": {"charts/unique_values_for_marvelous_inventor.py:chart:python:unique values for marvelous inventor": {"content": "columns = df_1.columns\nnumber_of_unique_values = [df_1[col].nunique() for col in columns]\n", "file_path": "charts/unique_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "unique_values_for_marvelous_inventor"}, "charts/missing_values_for_marvelous_inventor.py:chart:python:missing values for marvelous inventor": {"content": "number_of_rows = len(df_1.index)\ncolumns_with_mising_values = []\npercentage_of_missing_values = []\nfor col in df_1.columns:\n    missing = df_1[col].isna().sum()\n    if missing > 0:\n        columns_with_mising_values.append(col)\n        percentage_of_missing_values.append(100 * missing / number_of_rows)\n", "file_path": "charts/missing_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "missing_values_for_marvelous_inventor"}, "data_exporters/revered_grace.py:data_exporter:python:revered grace": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = kwargs['bigquery_table_id']\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/revered_grace.py", "language": "python", "type": "data_exporter", "uuid": "revered_grace"}, "data_exporters/autumn_illusion.py:data_exporter:python:autumn illusion": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-daatascience.your_dataset.your_table_name'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    print(config_path)\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/autumn_illusion.py", "language": "python", "type": "data_exporter", "uuid": "autumn_illusion"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/victorious_inventor.yaml:data_loader:yaml:victorious inventor": {"content": "config:\n  columns: null\n  headers: null\n  method: GET\n  payload: null\n  query: null\n  response_parser: null\n  url: null\nsource: api\n", "file_path": "data_loaders/victorious_inventor.yaml", "language": "yaml", "type": "data_loader", "uuid": "victorious_inventor"}, "data_loaders/load_econ.py:data_loader:python:load econ": {"content": "import io\nfrom fredapi import Fred\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\nfrom fredapi import Fred\nimport numpy as np\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n# Configuration\nFRB_API_KEY = get_secret_value('Fred_API')\n\n\n# Define what we want to download and their mappings\nFRB_RATES = ['DGS10', 'DGS2', 'DGS3', 'DGS1', 'DGS3MO', 'DGS6MO']\n\nFRB_SERIES_MAPPING = {\n    'CPIAUCSL': 'CPI',\n    'PCUOMFGOMFG': 'ppi', \n    'UNRATE': 'unemprate',\n    'CIVPART': 'laborpart',\n    'INDPRO': 'indprod',\n    'DFF': 'ffr',\n    'T10Y3M': 'ycurve10y3m',\n    'T10Y2Y': 'ycurve10y2y',\n    'DAAA': 'moodycorpbondyield'\n}\n\nFRB_INDICES_MAPPING = {\n    'SP500': 'snp500'\n}\n\n\ndef download_fred_data(series_codes: list, api_key: str = FRB_API_KEY, start_date: str = \"2010-01-01\") -> pd.DataFrame:\n    \"\"\"Download multiple FRED series and return as DataFrame.\"\"\"\n    fred = Fred(api_key=api_key)\n    data_dict = {}\n    \n    for code in series_codes:\n        try:\n            print(f\"Downloading {code}\")\n            series = fred.get_series(code)\n            filtered_series = series[series.index >= start_date]\n            data_dict[code] = filtered_series\n        except Exception as e:\n            print(f\"Error downloading {code}: {e}\")\n    \n    if data_dict:\n        df = pd.concat(data_dict, axis=1)\n        df.reset_index(inplace=True)\n        df.rename(columns={'index': 'DATE'}, inplace=True)\n        return df\n    else:\n        return pd.DataFrame()\n\n\ndef rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Rename columns to match desired output format.\"\"\"\n    # Rename rate columns (convert to lowercase for some)\n    rate_renames = {\n        'DGS3MO': 'DGS3mo',\n        'DGS6MO': 'DGS6mo'\n    }\n    \n    # Combine all rename mappings\n    all_renames = {**rate_renames, **FRB_SERIES_MAPPING, **FRB_INDICES_MAPPING}\n    \n    return df.rename(columns=all_renames)\n\n\ndef calculate_derived_variables(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Calculate derived/interpolated variables.\"\"\"\n    df = df.copy()\n    \n    # Convert DGS values from percentages to decimals\n    dgs_cols = ['DGS10', 'DGS2', 'DGS3', 'DGS1', 'DGS3mo', 'DGS6mo']\n    for col in dgs_cols:\n        if col in df.columns:\n            df[col] = df[col] / 100\n    \n    # Forward fill S&P 500 data\n    df['snp500'] = df['snp500'].fillna(method='ffill')\n    \n    # Interpolated rates (now in decimal form)\n    df['dgs1p5'] = (df['DGS1'] + df['DGS2']) / 2  # 1.5-Year Rate\n    df['dgs2p5'] = (df['DGS2'] + df['DGS3']) / 2  # 2.5-Year Rate  \n    df['dgs9mo'] = (df['DGS6mo'] + df['DGS1']) / 2  # 9-Month Rate\n    df['dgs15mo'] = (df['dgs1p5'] + df['DGS1']) / 2  # 15-Month Rate\n    \n    # S&P 500 log returns (calculated after forward filling)\n    df['snp500rets'] = np.log(df['snp500']).diff()\n    \n    return df\n\n@data_loader\ndef main() -> pd.DataFrame:\n    \"\"\"Download and process all financial data.\"\"\"\n    print(\"Downloading FRB rates...\")\n    rates_df = download_fred_data(FRB_RATES)\n    \n    print(\"\\nDownloading FRB economic series...\")\n    series_df = download_fred_data(list(FRB_SERIES_MAPPING.keys()))\n    \n    print(\"\\nDownloading indices...\")\n    indices_df = download_fred_data(list(FRB_INDICES_MAPPING.keys()))\n    \n    # Merge all dataframes\n    print(\"\\nMerging data...\")\n    dataframes = [df for df in [rates_df, series_df, indices_df] if not df.empty]\n    \n    if not dataframes:\n        print(\"No data downloaded\")\n        return pd.DataFrame()\n    \n    # Start with first dataframe and merge others\n    merged = dataframes[0].copy()\n    for df in dataframes[1:]:\n        merged = pd.merge(merged, df, on='DATE', how='outer')\n    \n    # Rename columns to match desired format\n    merged = rename_columns(merged)\n    \n    # Calculate derived variables\n    print(\"Calculating derived variables...\")\n    merged = calculate_derived_variables(merged)\n    \n    # Sort by date\n    merged.sort_values('DATE', inplace=True)\n    merged.reset_index(drop=True, inplace=True)\n    \n    print(f\"\\nDownload complete! Dataset shape: {merged.shape}\")\n    print(f\"Date range: {merged['DATE'].min()} to {merged['DATE'].max()}\")\n    \n    return merged\n", "file_path": "data_loaders/load_econ.py", "language": "python", "type": "data_loader", "uuid": "load_econ"}, "data_loaders/quixotic_core.py:data_loader:python:quixotic core": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/quixotic_core.py", "language": "python", "type": "data_loader", "uuid": "quixotic_core"}, "data_loaders/verdant_phoenix.py:data_loader:python:verdant phoenix": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    query = 'your_gbq_query'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    return BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/verdant_phoenix.py", "language": "python", "type": "data_loader", "uuid": "verdant_phoenix"}, "data_loaders/marvelous_inventor.py:data_loader:python:marvelous inventor": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nimport intrinio_sdk as intrinio\nfrom intrinio_sdk.rest import ApiException\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n#get Intrinio API Key secret\nAPI_KEY = get_secret_value('Intrinio_API')\n\ntoday = datetime.now()\nfive_years_ago = (today - relativedelta(years=3)).strftime(\"%Y-%m-%d\")\nresults_dictionary = {}\n\nintrinio.ApiClient().set_api_key(API_KEY) \nintrinio.ApiClient().allow_retries(True)\n\ndef get_universe():\n    date = '2025-05-14'  # Yesterday's date\n    page_size = 100\n    start_time = datetime.now()\n\n    # Get Marketcap data with pagination\n    marketcap_data = []\n    next_page = ''\n\n    while True:\n        try:\n            response = intrinio.CompanyApi().get_all_companies_daily_metrics(\n                on_date=date, \n                page_size=page_size,\n                next_page=next_page\n            )\n            \n            print(f'Processing {len(response.daily_metrics)} marketcap entries...')\n            \n            # Process current page of results\n            for daily_metric in response.daily_metrics:\n                ticker = daily_metric.company.ticker\n                marketcap = daily_metric.market_cap\n                \n                data = {\n                    \"ticker\" : ticker, \n                    \"name\": daily_metric.company.name,\n                    \"date\": daily_metric.date.strftime(\"%Y-%m-%d\"),\n                    \"marketcap\": marketcap\n                }\n                \n                marketcap_data.append(data)\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        except ApiException as e:\n            print(f\"Exception when calling CompanyApi->get_all_companies_daily_metrics: {e}\")\n            break\n\n    print(f'Found {len(marketcap_data)} marketcap entries for: {date}')\n    print(f'Time elapsed: {datetime.now() - start_time}')\n\n    universe = pd.DataFrame(list(marketcap_data))\n    selected_universe = universe[((universe.marketcap>2000000000) & (universe.ticker.notna()))]\n    \n    return selected_universe\n\ndef work(ticker):\n    # Get EOD Stock Prices with pagination\n    # https://docs.intrinio.com/documentation/python/get_security_stock_prices_v2\n    identifier = ticker\n    start_date = five_years_ago\n    page_size = 100  # Maximum allowed page size\n    total_prices = 0\n    \n    try:\n        # Initialize pagination\n        next_page = ''\n        \n        while True:\n            # Get the current page of results\n            response = intrinio.SecurityApi().get_security_stock_prices(\n                identifier, \n                start_date=start_date, \n                page_size=page_size, \n                next_page=next_page\n            )\n            \n            security = response.security\n            page_prices = len(response.stock_prices)\n            total_prices += page_prices\n            \n            # Process the current page of stock price data\n            for stock_price in response.stock_prices:\n                key = f'{ticker}|{stock_price.date}'\n                data = {\n                    'security_id': security.id,\n                    'company_id': security.company_id,\n                    'ticker': security.ticker,\n                    'date': stock_price.date,\n                    'open': stock_price.open,\n                    'high': stock_price.high,\n                    'low': stock_price.low,\n                    \"close\": stock_price.close,\n                    'adj_open': stock_price.adj_open,\n                    'adj_high': stock_price.adj_high,\n                    \"adj_low\": stock_price.adj_low,\n                    \"adj_close\": stock_price.adj_close,\n                    'adj_volume':stock_price.adj_volume,\n                    'fifty_two_week_high' : stock_price.fifty_two_week_high,\n                    'fifty_two_week_low':stock_price.fifty_two_week_low,\n                    'dividend': stock_price.dividend\n                }\n                results_dictionary[key] = data\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        print(f'Found {total_prices} prices for: {ticker}')\n        \n    except ApiException as e:\n        print(f\"Exception when calling SecurityApi->get_security_stock_prices for {ticker}: {e}\")\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n\n    start_time = datetime.now()\n\n    selected_universe = get_universe()\n    tickers = selected_universe.ticker.tolist()\n    tickers = ['AAPL', 'IBM'] #DELETE AFTER TESTING\n    print(f'Loading {len(tickers)} tickers') \n    print(tickers)\n\n    for ticker in tqdm(tickers):\n        work(ticker)\n \n    df = pd.DataFrame(list(results_dictionary.values()))\n    print(len(df.ticker.unique()))\n    \n    return(df)", "file_path": "data_loaders/marvelous_inventor.py", "language": "python", "type": "data_loader", "uuid": "marvelous_inventor"}, "data_loaders/icy_echo.py:data_loader:python:icy echo": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/icy_echo.py", "language": "python", "type": "data_loader", "uuid": "icy_echo"}, "transformers/resilient_sword.py:transformer:python:resilient sword": {"content": "import pandas as pd\nimport numpy as np\nfrom typing import List, Dict\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef calculate_forward_std_devs_fast(df) -> pd.DataFrame:\n    \"\"\"Faster vectorized version using rolling windows.\"\"\"\n    \n    horizons = {'3mo': 63, '6mo': 126, '9mo': 189, '12mo': 252, \n                '15mo': 315, '18mo': 378, '24mo': 504}\n    \n    df = df.sort_values(['ticker', 'date']).copy()\n    \n    # Initialize all std columns\n    for period in horizons:\n        df[f'std{period}sfwds'] = np.nan\n    \n    # Process each ticker\n    for ticker in df['ticker'].unique():\n        mask = df['ticker'] == ticker\n        ticker_data = df.loc[mask, 'adj_close'].values\n        n_obs = len(ticker_data)\n        \n        # Calculate for each horizon\n        for period, window in horizons.items():\n            std_values = np.full(n_obs, np.nan)\n            \n            for i in range(n_obs):\n                end_idx = min(n_obs, i + window)\n                if end_idx - i > 2:  # Need more than 2 observations\n                    std_values[i] = np.std(ticker_data[i:end_idx], ddof=1)\n            \n            df.loc[mask, f'std{period}sfwds'] = std_values\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/resilient_sword.py", "language": "python", "type": "transformer", "uuid": "resilient_sword"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/block1.py:transformer:python:block1": {"content": "from mage_ai.data_cleaner.transformer_actions.constants import ImputationStrategy\nfrom mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\nimport numpy as np\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef add_forward_weights(df, *args, **kwargs):\n    \"\"\"\n    Add weight columns for forward-looking analysis periods to handle edge effects.\n    \n    Parameters:\n    -----------\n    df : pandas.DataFrame\n        DataFrame with daily data and datetime index\n    timeframes_months : list, optional\n        List of timeframes in months. Default: [3, 6, 9, 12, 15, 18, 24]\n    trading_days_per_month : int, optional\n        Trading days per month. Default: 21 (252 trading days / 12 months)\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        Original DataFrame with added weight columns (wgt3mos, wgt6mos, etc.)\n    \"\"\"\n    trading_days_per_month = kwargs['configuration'].get('trading_days_per_month')\n    timeframes_months = kwargs['configuration'].get('timeframes_months')\n\n    if timeframes_months is None:\n        timeframes_months = [3, 6, 9, 12, 15, 18, 24]\n    \n    # Create a copy to avoid modifying original\n    result_df = df.copy()\n    \n    # Total number of observations\n    total_obs = len(df)\n    \n    # Calculate weights for each timeframe\n    for months in timeframes_months:\n        # Convert months to trading days\n        tau = months * trading_days_per_month\n        \n        # Create weight column name\n        weight_col = f'wgt{months}mos'\n        \n        # Calculate weights for each observation\n        weights = []\n        \n        for i in range(total_obs):\n            # Calculate end point of forward window\n            end_point = min(i + tau - 1, total_obs - 1)\n            \n            # Calculate actual days available vs requested\n            days_available = end_point - i + 1\n            \n            # Weight = actual days / requested days\n            weight = days_available / tau\n            \n            weights.append(weight)\n        \n        # Add weight column to DataFrame\n        result_df[weight_col] = weights\n    print(result_df.tail(50))\n    return result_df", "file_path": "transformers/block1.py", "language": "python", "type": "transformer", "uuid": "block1"}, "transformers/barrier_metrics.py:transformer:python:barrier metrics": {"content": "import pandas as pd\nimport numpy as np\nfrom typing import List, Dict\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef calculate_barrier_metrics(df, *args, **kwargs\n) -> pd.DataFrame:\n    \"\"\"\n    Calculate comprehensive barrier breach metrics for panel stock data.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Panel data with columns: ['date', 'ticker', 'adjusted_close']\n        Must be sorted by ticker, then date\n    barriers : List[float]\n        List of barrier levels as fractions (e.g., [0.85, 0.80, 0.75, 0.70])\n    trading_days_per_year : int, default 252\n        Number of trading days per year for timeframe conversion\n    timeframes_months : List[int], default [3, 6, 9, 12, 15, 18, 24]\n        Timeframes in months to calculate\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Original dataframe with additional columns:\n        - mean_bb_price{timeframe}mos{barrier_num}: Mean price during barrier breaches\n        - pct_above{timeframe}mos{barrier_num}: Percentage of days above barrier\n        - pct_below{timeframe}mos{barrier_num}: Percentage of days below barrier\n    \"\"\"\n    \n    # Validate inputs\n    \n    required_cols = ['date', 'ticker', 'adj_close']\n    if not all(col in df.columns for col in required_cols):\n        raise ValueError(f\"DataFrame must contain columns: {required_cols}\")\n    \n    barriers = kwargs['configuration'].get('barriers')\n\n    if not barriers:\n        raise ValueError(\"At least one barrier level must be provided\")\n    \n    timeframes_months = kwargs['configuration'].get('timeframes_months')\n\n    trading_days_per_year = kwargs['configuration'].get('trading_days_per_year')\n    # Convert timeframes to trading days\n    timeframes_days = {\n        months: int(months * trading_days_per_year / 12) \n        for months in timeframes_months\n    }\n    \n    # Ensure data is sorted\n    df_sorted = df.sort_values(['ticker', 'date']).reset_index(drop=True)\n    \n    # Initialize result columns\n    result_cols = {}\n    for months in timeframes_months:\n        for i, barrier in enumerate(barriers, 1):\n            result_cols[f'mean_bb_price{months}mos{i}'] = [np.nan] * len(df_sorted)\n            result_cols[f'pct_above{months}mos{i}'] = [np.nan] * len(df_sorted)\n            result_cols[f'pct_below{months}mos{i}'] = [np.nan] * len(df_sorted)\n    \n    # Group by ticker for panel processing\n    grouped = df_sorted.groupby('ticker')\n    \n    for ticker, group in grouped:\n        print(f\"Processing {ticker}...\")\n        \n        # Convert to numpy arrays for faster computation\n        prices = group['adj_close'].values\n        ticker_indices = group.index.values\n        n_obs = len(prices)\n        \n        # For each observation date\n        for i in range(n_obs):\n            current_price = prices[i]\n            \n            # Calculate barrier prices for this observation\n            barrier_prices = [current_price * barrier for barrier in barriers]\n            \n            # For each timeframe\n            for months in timeframes_months:\n                days_ahead = timeframes_days[months]\n                \n                # Define forward-looking window - SAS style: start from i (inclusive)\n                # toploop = min(observations, target_days) but from current position\n                remaining_obs = n_obs - i\n                actual_window_size = min(remaining_obs, days_ahead)\n                \n                # Extract forward-looking prices INCLUDING current observation (i)\n                # This matches SAS: do j=i to toploop where j starts at i\n                forward_prices = prices[i:i + actual_window_size]\n                total_days = len(forward_prices)\n                \n                # Skip if no forward data\n                if total_days == 0:\n                    continue\n                \n                original_idx = ticker_indices[i]\n                \n                # For each barrier level\n                for barrier_idx, barrier_price in enumerate(barrier_prices, 1):\n                    \n                    # Calculate above/below barrier masks\n                    above_barrier_mask = forward_prices >= forward_prices[0]\n                    below_barrier_mask = forward_prices < barrier_price\n                    \n                    # Calculate percentages\n                    days_above = np.sum(above_barrier_mask)\n                    days_below = np.sum(below_barrier_mask)\n                    \n                    pct_above = days_above / total_days if total_days > 0 else 0\n                    pct_below = days_below / total_days if total_days > 0 else 0\n                    \n                    # Store percentage results\n                    result_cols[f'pct_above{months}mos{barrier_idx}'][original_idx] = pct_above\n                    result_cols[f'pct_below{months}mos{barrier_idx}'][original_idx] = pct_below\n                    \n                    # Calculate mean breach price (only if there were breaches)\n                    if days_below > 0:\n                        below_barrier_prices = forward_prices[below_barrier_mask]\n                        mean_bb_price = np.mean(below_barrier_prices)\n                        result_cols[f'mean_bb_price{months}mos{barrier_idx}'][original_idx] = mean_bb_price\n    \n    # Add calculated columns to original dataframe\n    result_df = df_sorted.copy()\n    for col_name, values in result_cols.items():\n        result_df[col_name] = values\n    \n    return result_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/barrier_metrics.py", "language": "python", "type": "transformer", "uuid": "barrier_metrics"}, "transformers/solitary_frog.py:transformer:python:solitary frog": {"content": "# merge_deduplicate_flexible_block.py\nimport pandas as pd\n\n@transformer\ndef merge_and_deduplicate_flexible(*dataframes_to_merge, **kwargs):\n    \"\"\"\n    Merges data from an arbitrary number of upstream transformer blocks\n    and drops duplicates.\n\n    Args:\n        *dataframes_to_merge: A variable number of pd.DataFrame objects\n                              passed as positional arguments from upstream blocks.\n        **kwargs: Standard keyword arguments provided by Mage (e.g., execution_date).\n\n    Returns:\n        pd.DataFrame: Merged and deduplicated DataFrame.\n    \"\"\"\n    if not dataframes_to_merge:\n        print(\"No DataFrames received for merging. Returning empty DataFrame.\")\n        return pd.DataFrame()\n\n    print(f\"Received {len(dataframes_to_merge)} DataFrames for merging.\")\n\n    # Log shapes of incoming DataFrames for debugging\n    for i, df in enumerate(dataframes_to_merge):\n        if isinstance(df, pd.DataFrame):\n            print(f\"  DataFrame {i+1} shape: {df.shape}\")\n        else:\n            print(f\"  Warning: Argument {i+1} is not a DataFrame: {type(df)}\")\n\n    # 1. Merge the data\n    # Use pd.concat to stack all DataFrames\n    # It's robust to different column sets, filling missing with NaN\n    try:\n        merged_df = pd.concat(list(dataframes_to_merge), ignore_index=True)\n        print(\"Shape after concatenation:\", merged_df.shape)\n    except Exception as e:\n        print(f\"Error during concatenation: {e}\")\n        # Depending on your error strategy, you might want to re-raise or return empty\n        return pd.DataFrame()\n\n    # 2. Drop Duplicates\n    # You still need to decide what constitutes a \"duplicate\".\n    # Assuming dropping based on all columns for now.\n    # Adjust `subset=['col1', 'col2']` if you need specific column-based deduplication.\n    initial_rows = merged_df.shape[0]\n    deduplicated_df = merged_df.drop_duplicates()\n    final_rows = deduplicated_df.shape[0]\n\n    print(f\"Shape after deduplication: {deduplicated_df.shape}\")\n    print(f\"Dropped {initial_rows - final_rows} duplicate rows.\")\n\n    return deduplicated_df", "file_path": "transformers/solitary_frog.py", "language": "python", "type": "transformer", "uuid": "solitary_frog"}, "transformers/interpolate_ffill.py:transformer:python:interpolate ffill": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.DIFF\n\n    Calculates difference from previous row along column.\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#difference\n    \"\"\"\n    df_filled = df.ffill()\n\n    return df_filled\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/interpolate_ffill.py", "language": "python", "type": "transformer", "uuid": "interpolate_ffill"}, "pipelines/data_economics/interactions.yaml:pipeline:yaml:data economics/interactions": {"content": "blocks: {}\n", "file_path": "pipelines/data_economics/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_economics/interactions"}, "pipelines/data_economics/__init__.py:pipeline:python:data economics/  init  ": {"content": "", "file_path": "pipelines/data_economics/__init__.py", "language": "python", "type": "pipeline", "uuid": "data_economics/__init__"}, "pipelines/data_economics/metadata.yaml:pipeline:yaml:data economics/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/load_econ.py\n    file_source:\n      path: data_loaders/load_econ.py\n  downstream_blocks:\n  - interpolate_ffill\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_econ\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_econ\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/interpolate_ffill.py\n    file_source:\n      path: transformers/interpolate_ffill.py\n  downstream_blocks:\n  - block1\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: interpolate_ffill\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_econ\n  uuid: interpolate_ffill\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/block1.py\n    file_source:\n      path: transformers/block1.py\n    timeframes_months: null\n    trading_days_per_month: 21\n  downstream_blocks:\n  - revered_grace\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: block1\n  retry_config: {}\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - interpolate_ffill\n  uuid: block1\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/revered_grace.py\n    file_source:\n      path: data_exporters/revered_grace.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: revered_grace\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - block1\n  uuid: revered_grace\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-17 17:02:49.333987+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: data_economics\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: data_economics\nvariables:\n  bigquery_table_id: scg-datascience.scg_stocks.econ\nvariables_dir: /home/src/mage_data/nexus\nwidgets: []\n", "file_path": "pipelines/data_economics/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_economics/metadata"}, "pipelines/data_stockeod/interactions.yaml:pipeline:yaml:data stockeod/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "pipelines/data_stockeod/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_stockeod/interactions"}, "pipelines/data_stockeod/__init__.py:pipeline:python:data stockeod/  init  ": {"content": "", "file_path": "pipelines/data_stockeod/__init__.py", "language": "python", "type": "pipeline", "uuid": "data_stockeod/__init__"}, "pipelines/data_stockeod/metadata.yaml:pipeline:yaml:data stockeod/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - missing_values_for_marvelous_inventor\n  - unique_values_for_marvelous_inventor\n  - resilient_sword\n  - barrier_metrics\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: marvelous inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: marvelous_inventor\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - solitary_frog\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: resilient sword\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: resilient_sword\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    barriers:\n    - 0.55\n    - 0.75\n    - 0.75\n    - 0.85\n    file_path: transformers/barrier_metrics.py\n    file_source:\n      path: transformers/barrier_metrics.py\n    timeframes_months:\n    - 3\n    - 6\n    - 9\n    - 12\n    - 15\n    - 18\n    - 24\n    trading_days_per_year: 252\n  downstream_blocks:\n  - solitary_frog\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: barrier_metrics\n  retry_config: {}\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: barrier_metrics\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - revered_grace\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: solitary frog\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - barrier_metrics\n  - resilient_sword\n  uuid: solitary_frog\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dynamic: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: revered grace\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - solitary_frog\n  uuid: revered_grace\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-14 17:20:18.207723+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: data_stockEOD\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: data_stockeod\nvariables:\n  bigquery_table_id: scg-datascience.scg_stocks.EOD\nvariables_dir: /home/src/mage_data/nexus\nwidgets:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    group_by: []\n    x: columns_with_mising_values\n    y: percentage_of_missing_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: missing values for marvelous_inventor\n  retry_config: null\n  status: failed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: missing_values_for_marvelous_inventor\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    x: columns\n    y: number_of_unique_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: unique values for marvelous_inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: unique_values_for_marvelous_inventor\n", "file_path": "pipelines/data_stockeod/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "data_stockeod/metadata"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}
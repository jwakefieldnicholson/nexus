{"block_file": {"charts/missing_values_for_marvelous_inventor.py:chart:python:missing values for marvelous inventor": {"content": "number_of_rows = len(df_1.index)\ncolumns_with_mising_values = []\npercentage_of_missing_values = []\nfor col in df_1.columns:\n    missing = df_1[col].isna().sum()\n    if missing > 0:\n        columns_with_mising_values.append(col)\n        percentage_of_missing_values.append(100 * missing / number_of_rows)\n", "file_path": "charts/missing_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "missing_values_for_marvelous_inventor"}, "charts/unique_values_for_marvelous_inventor.py:chart:python:unique values for marvelous inventor": {"content": "columns = df_1.columns\nnumber_of_unique_values = [df_1[col].nunique() for col in columns]\n", "file_path": "charts/unique_values_for_marvelous_inventor.py", "language": "python", "type": "chart", "uuid": "unique_values_for_marvelous_inventor"}, "data_exporters/autumn_illusion.py:data_exporter:python:autumn illusion": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-daatascience.your_dataset.your_table_name'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    print(config_path)\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/autumn_illusion.py", "language": "python", "type": "data_exporter", "uuid": "autumn_illusion"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/revered_grace.py:data_exporter:python:revered grace": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-datascience.scg_stocks.EOD'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "data_exporters/revered_grace.py", "language": "python", "type": "data_exporter", "uuid": "revered_grace"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/marvelous_inventor.py:data_loader:python:marvelous inventor": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nimport intrinio_sdk as intrinio\nfrom intrinio_sdk.rest import ApiException\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\n#get Intrinio API Key secret\nAPI_KEY = get_secret_value('Intrinio_API')\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\ntoday = datetime.now()\nfive_years_ago = (today - relativedelta(years=3)).strftime(\"%Y-%m-%d\")\nresults_dictionary = {}\n\nintrinio.ApiClient().set_api_key(API_KEY) \nintrinio.ApiClient().allow_retries(True)\n\ndef get_universe():\n    date = '2025-05-14'  # Yesterday's date\n    page_size = 100\n    start_time = datetime.now()\n\n    # Get Marketcap data with pagination\n    marketcap_data = []\n    next_page = ''\n\n    while True:\n        try:\n            response = intrinio.CompanyApi().get_all_companies_daily_metrics(\n                on_date=date, \n                page_size=page_size,\n                next_page=next_page\n            )\n            \n            print(f'Processing {len(response.daily_metrics)} marketcap entries...')\n            \n            # Process current page of results\n            for daily_metric in response.daily_metrics:\n                ticker = daily_metric.company.ticker\n                marketcap = daily_metric.market_cap\n                \n                data = {\n                    \"ticker\" : ticker, \n                    \"name\": daily_metric.company.name,\n                    \"date\": daily_metric.date.strftime(\"%Y-%m-%d\"),\n                    \"marketcap\": marketcap\n                }\n                \n                marketcap_data.append(data)\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        except ApiException as e:\n            print(f\"Exception when calling CompanyApi->get_all_companies_daily_metrics: {e}\")\n            break\n\n    print(f'Found {len(marketcap_data)} marketcap entries for: {date}')\n    print(f'Time elapsed: {datetime.now() - start_time}')\n\n    universe = pd.DataFrame(list(marketcap_data))\n    selected_universe = universe[((universe.marketcap>2000000000) & (universe.ticker.notna()))]\n    \n    return selected_universe\n\ndef work(ticker):\n    # Get EOD Stock Prices with pagination\n    # https://docs.intrinio.com/documentation/python/get_security_stock_prices_v2\n    identifier = ticker\n    start_date = five_years_ago\n    page_size = 100  # Maximum allowed page size\n    total_prices = 0\n    \n    try:\n        # Initialize pagination\n        next_page = ''\n        \n        while True:\n            # Get the current page of results\n            response = intrinio.SecurityApi().get_security_stock_prices(\n                identifier, \n                start_date=start_date, \n                page_size=page_size, \n                next_page=next_page\n            )\n            \n            security = response.security\n            page_prices = len(response.stock_prices)\n            total_prices += page_prices\n            \n            # Process the current page of stock price data\n            for stock_price in response.stock_prices:\n                key = f'{ticker}|{stock_price.date}'\n                data = {\n                    'security_id': security.id,\n                    'company_id': security.company_id,\n                    'ticker': security.ticker,\n                    'date': stock_price.date,\n                    'open': stock_price.open,\n                    'high': stock_price.high,\n                    'low': stock_price.low,\n                    \"close\": stock_price.close,\n                    'adj_open': stock_price.adj_open,\n                    'adj_high': stock_price.adj_high,\n                    \"adj_low\": stock_price.adj_low,\n                    \"adj_close\": stock_price.adj_close,\n                    'fifty_two_week_high' : stock_price.fifty_two_week_high,\n                    'fifty_two_week_low':stock_price.fifty_two_week_low,\n                    'dividend': stock_price.dividend\n                }\n                results_dictionary[key] = data\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        print(f'Found {total_prices} prices for: {ticker}')\n        \n    except ApiException as e:\n        print(f\"Exception when calling SecurityApi->get_security_stock_prices for {ticker}: {e}\")\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n\n    start_time = datetime.now()\n\n    selected_universe = get_universe()\n    tickers = selected_universe.ticker.tolist()\n    print(f'Loading {len(tickers)} tickers') \n    print(tickers)\n\n    for ticker in tqdm(tickers):\n        work(ticker)\n \n    df = pd.DataFrame(list(results_dictionary.values()))\n    print(len(df.ticker.unique()))\n    \n    return(df)", "file_path": "data_loaders/marvelous_inventor.py", "language": "python", "type": "data_loader", "uuid": "marvelous_inventor"}, "data_loaders/victorious_inventor.yaml:data_loader:yaml:victorious inventor": {"content": "config:\n  columns: null\n  headers: null\n  method: GET\n  payload: null\n  query: null\n  response_parser: null\n  url: null\nsource: api\n", "file_path": "data_loaders/victorious_inventor.yaml", "language": "yaml", "type": "data_loader", "uuid": "victorious_inventor"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/lucid_sun/__init__.py:pipeline:python:lucid sun/  init  ": {"content": "", "file_path": "pipelines/lucid_sun/__init__.py", "language": "python", "type": "pipeline", "uuid": "lucid_sun/__init__"}, "pipelines/lucid_sun/interactions.yaml:pipeline:yaml:lucid sun/interactions": {"content": "blocks: {}\n", "file_path": "pipelines/lucid_sun/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "lucid_sun/interactions"}, "pipelines/lucid_sun/metadata.yaml:pipeline:yaml:lucid sun/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - revered_grace\n  - missing_values_for_marvelous_inventor\n  - unique_values_for_marvelous_inventor\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: marvelous inventor\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: marvelous_inventor\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dynamic: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: revered grace\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: revered_grace\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-14 17:20:18.207723+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: lucid sun\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: lucid_sun\nvariables_dir: /home/src/mage_data/nexus\nwidgets:\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    group_by: []\n    x: columns_with_mising_values\n    y: percentage_of_missing_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: missing values for marvelous_inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: missing_values_for_marvelous_inventor\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    chart_style: horizontal\n    chart_type: bar chart\n    x: columns\n    y: number_of_unique_values\n    y_sort_order: descending\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: unique values for marvelous_inventor\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - marvelous_inventor\n  uuid: unique_values_for_marvelous_inventor\n", "file_path": "pipelines/lucid_sun/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "lucid_sun/metadata"}, "/home/src/nexus/data_exporters/revered_grace.py:data_exporter:python:home/src/nexus/data exporters/revered grace": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'scg-datascience.scg_stocks.EOD'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )", "file_path": "/home/src/nexus/data_exporters/revered_grace.py", "language": "python", "type": "data_exporter", "uuid": "revered_grace"}, "/home/src/nexus/data_loaders/marvelous_inventor.py:data_loader:python:home/src/nexus/data loaders/marvelous inventor": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nimport intrinio_sdk as intrinio\nfrom intrinio_sdk.rest import ApiException\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport concurrent.futures\nfrom tqdm import tqdm\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n#get Intrinio API Key secret\nAPI_KEY = get_secret_value('Intrinio_API')\n\ntoday = datetime.now()\nfive_years_ago = (today - relativedelta(years=3)).strftime(\"%Y-%m-%d\")\nresults_dictionary = {}\n\nintrinio.ApiClient().set_api_key(API_KEY) \nintrinio.ApiClient().allow_retries(True)\n\ndef get_universe():\n    date = '2025-05-14'  # Yesterday's date\n    page_size = 100\n    start_time = datetime.now()\n\n    # Get Marketcap data with pagination\n    marketcap_data = []\n    next_page = ''\n\n    while True:\n        try:\n            response = intrinio.CompanyApi().get_all_companies_daily_metrics(\n                on_date=date, \n                page_size=page_size,\n                next_page=next_page\n            )\n            \n            print(f'Processing {len(response.daily_metrics)} marketcap entries...')\n            \n            # Process current page of results\n            for daily_metric in response.daily_metrics:\n                ticker = daily_metric.company.ticker\n                marketcap = daily_metric.market_cap\n                \n                data = {\n                    \"ticker\" : ticker, \n                    \"name\": daily_metric.company.name,\n                    \"date\": daily_metric.date.strftime(\"%Y-%m-%d\"),\n                    \"marketcap\": marketcap\n                }\n                \n                marketcap_data.append(data)\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        except ApiException as e:\n            print(f\"Exception when calling CompanyApi->get_all_companies_daily_metrics: {e}\")\n            break\n\n    print(f'Found {len(marketcap_data)} marketcap entries for: {date}')\n    print(f'Time elapsed: {datetime.now() - start_time}')\n\n    universe = pd.DataFrame(list(marketcap_data))\n    selected_universe = universe[((universe.marketcap>2000000000) & (universe.ticker.notna()))]\n    \n    return selected_universe\n\ndef work(ticker):\n    # Get EOD Stock Prices with pagination\n    # https://docs.intrinio.com/documentation/python/get_security_stock_prices_v2\n    identifier = ticker\n    start_date = five_years_ago\n    page_size = 100  # Maximum allowed page size\n    total_prices = 0\n    \n    try:\n        # Initialize pagination\n        next_page = ''\n        \n        while True:\n            # Get the current page of results\n            response = intrinio.SecurityApi().get_security_stock_prices(\n                identifier, \n                start_date=start_date, \n                page_size=page_size, \n                next_page=next_page\n            )\n            \n            security = response.security\n            page_prices = len(response.stock_prices)\n            total_prices += page_prices\n            \n            # Process the current page of stock price data\n            for stock_price in response.stock_prices:\n                key = f'{ticker}|{stock_price.date}'\n                data = {\n                    'security_id': security.id,\n                    'company_id': security.company_id,\n                    'ticker': security.ticker,\n                    'date': stock_price.date,\n                    'open': stock_price.open,\n                    'high': stock_price.high,\n                    'low': stock_price.low,\n                    \"close\": stock_price.close,\n                    'adj_open': stock_price.adj_open,\n                    'adj_high': stock_price.adj_high,\n                    \"adj_low\": stock_price.adj_low,\n                    \"adj_close\": stock_price.adj_close,\n                    'adj_volume':stock_price.adj_volume,\n                    'fifty_two_week_high' : stock_price.fifty_two_week_high,\n                    'fifty_two_week_low':stock_price.fifty_two_week_low,\n                    'dividend': stock_price.dividend\n                }\n                results_dictionary[key] = data\n            \n            # Check if there are more pages\n            next_page = response.next_page\n            if not next_page:\n                break\n                \n        print(f'Found {total_prices} prices for: {ticker}')\n        \n    except ApiException as e:\n        print(f\"Exception when calling SecurityApi->get_security_stock_prices for {ticker}: {e}\")\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n\n    start_time = datetime.now()\n\n    selected_universe = get_universe()\n    tickers = selected_universe.ticker.tolist()\n    \n    print(f'Loading {len(tickers)} tickers') \n    print(tickers)\n\n    for ticker in tqdm(tickers):\n        work(ticker)\n \n    df = pd.DataFrame(list(results_dictionary.values()))\n    print(len(df.ticker.unique()))\n    \n    return(df)", "file_path": "/home/src/nexus/data_loaders/marvelous_inventor.py", "language": "python", "type": "data_loader", "uuid": "marvelous_inventor"}, "/home/src/nexus/data_loaders/effortless_sound.py:data_loader:python:home/src/nexus/data loaders/effortless sound": {"content": "import io\nimport pandas as pd\nfrom fredapi import Fred\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\nimport pandas as pd\nfrom fredapi import Fred\n\ndef get_frb_series(series_codes, api_key, start_date='2010-01-01', end_date=None):\n    \"\"\"\n    Fetch multiple time series from the Federal Reserve Economic Data (FRED).\n    \n    Parameters:\n    -----------\n    series_codes : list\n        List of FRED series codes to retrieve\n    api_key : str\n        FRED API key\n    start_date : str, optional\n        Start date in 'YYYY-MM-DD' format\n    end_date : str, optional\n        End date in 'YYYY-MM-DD' format (default: None/today)\n    \n    Returns:\n    --------\n    pandas.DataFrame\n        DataFrame with all requested series as columns\n    \"\"\"\n    fred = Fred(api_key=api_key)\n    \n    try:\n        # Use dictionary comprehension to fetch all series\n        series_dict = {\n            code: fred.get_series(code, observation_start=start_date, observation_end=end_date)\n            for code in series_codes\n        }\n        \n        # Combine into a single DataFrame\n        return pd.DataFrame(series_dict)\n        \n    except Exception as e:\n        print(f\"Error fetching FRED data: {e}\")\n        return None\n\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/nexus/data_loaders/effortless_sound.py", "language": "python", "type": "data_loader", "uuid": "effortless_sound"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}